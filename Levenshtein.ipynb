{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae5e208",
   "metadata": {},
   "source": [
    "# Download vocabulary from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5ca436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0547defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/nico/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a027ba",
   "metadata": {},
   "source": [
    "# Define suggestion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e153a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_english_vocabulary(name='en'):\n",
    "    \"\"\"\n",
    "    Download and load the English vocabulary from NLTK.\n",
    "    \n",
    "    Return:\n",
    "        set: A set containing English words.\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(words.words(name))\n",
    "    vocabulary = {word.lower() for word in vocabulary}\n",
    "    print(f\"Vocabulary loaded with {len(vocabulary)} words.\")\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8ccc09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(token1, token2):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two tokens.\n",
    "    \n",
    "    Return:\n",
    "        int: The Levenshtein distance between the two tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = [[0] * (len(token2) + 1) for _ in range(len(token1) + 1)]\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if token1[t1 - 1] == token2[t2 - 1]:\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]      # insert\n",
    "                b = distances[t1 - 1][t2]      # delete\n",
    "                c = distances[t1 - 1][t2 - 1]  # replace\n",
    "                distances[t1][t2] = min(a, b, c) + 1\n",
    "\n",
    "    return distances[len(token1)][len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6fcc518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections(token, \n",
    "                        vocabulary, \n",
    "                        max_suggestions=5, \n",
    "                        max_distance=2):\n",
    "    \"\"\"\n",
    "    Suggest corrections for a given token based on the English vocabulary.\n",
    "    \n",
    "    Return:\n",
    "        List[str]: A list of suggested corrections for the token.\n",
    "    \"\"\"\n",
    "    # If the token is in the vocabulary, it is a right token \\(no correction needed)\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in vocabulary:\n",
    "        return [token]\n",
    "\n",
    "    # Return candidates with small Levenshtein distance\n",
    "    candidates = []\n",
    "    for word in vocabulary:\n",
    "        # Calculate Levenshtein distance only if the \\\n",
    "            # length difference is within the allowed range \n",
    "        # This helps to reduce the number of comparisons\n",
    "        if abs(len(token_lower) - len(word)) <= max_distance:\n",
    "            dist = levenshtein_distance(token_lower, word)\n",
    "            if dist <= max_distance:\n",
    "                candidates.append((word, dist))\n",
    "\n",
    "    candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "    return [word for word, dist in candidates[:max_suggestions]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fadea",
   "metadata": {},
   "source": [
    "# Testing with sample texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1709ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 234371 words.\n",
      "Suggestions for 'Ameraca': ['america', 'amerce', 'araca', 'maraca', 'amurca']\n",
      "Suggestions for 'bsn': ['bin', 'bosn', 'bon', 'ben', 'bun']\n",
      "Suggestions for 'hallo': ['callo', 'hall', 'halloo', 'hallow', 'hello']\n"
     ]
    }
   ],
   "source": [
    "english_vocabulary = load_english_vocabulary()\n",
    "test_words = ['Ameraca', 'bsn', 'hsllo']\n",
    "\n",
    "for test in test_words:\n",
    "    suggestions = suggest_corrections(test, english_vocabulary)\n",
    "    print(f\"Suggestions for '{test}': {suggestions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1e4725",
   "metadata": {},
   "source": [
    "# Calculate keyboard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36fafa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance matrix saved to 'keyboard_distances.csv'\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import pandas as pd\n",
    "\n",
    "def build_keyboard_graph():\n",
    "    \"\"\"\n",
    "    Create a graph (adjacency list) for the QWERTY keyboard.\n",
    "    Each key is a node, and its value is a set of adjacent keys.\n",
    "    \n",
    "    Return:\n",
    "        dict: A dictionary representing the keyboard graph.\n",
    "    \"\"\"\n",
    "    layout = [\n",
    "        \"qwertyuiop\",\n",
    "        \"asdfghjkl\",\n",
    "        \"zxcvbnm\"\n",
    "    ]\n",
    "    \n",
    "    graph = {char: set() for row in layout for char in row}\n",
    "    rows = len(layout)\n",
    "    \n",
    "    for r_idx, row in enumerate(layout):\n",
    "        cols = len(row)\n",
    "        for c_idx, char in enumerate(row):\n",
    "            # Add neighbors in the above row\n",
    "            if r_idx > 0 and c_idx < len(layout[r_idx - 1]):\n",
    "                neighbor = layout[r_idx - 1][c_idx]\n",
    "                graph[char].add(neighbor)\n",
    "                graph[neighbor].add(char)\n",
    "\n",
    "            # Add neighbors in the left\n",
    "            if c_idx > 0:\n",
    "                neighbor = layout[r_idx][c_idx - 1]\n",
    "                graph[char].add(neighbor)\n",
    "                graph[neighbor].add(char)\n",
    "                \n",
    "    return graph\n",
    "\n",
    "def bfs_from_start_node(graph, start_node):\n",
    "    \"\"\"\n",
    "    Run BFS from a start node to find distances to all other nodes.\n",
    "    \n",
    "    Input:\n",
    "        graph (dict): The keyboard graph.\n",
    "        start_node (str): The key from which to start the BFS.\n",
    "        \n",
    "    Return:\n",
    "        dict: A dictionary with distances from the start node to all other nodes.\n",
    "    \"\"\"\n",
    "    # Initialize distances with -1 (unvisited)\n",
    "    distances = {node: -1 for node in graph}\n",
    "\n",
    "    # Set the distance to the start node as 0\n",
    "    distances[start_node] = 0\n",
    "    \n",
    "    queue = deque([start_node])\n",
    "    \n",
    "    while queue:\n",
    "        current_node = queue.popleft()\n",
    "        \n",
    "        for neighbor in sorted(list(graph[current_node])):\n",
    "            if distances[neighbor] == -1: # If node is unvisited\n",
    "                distances[neighbor] = distances[current_node] + 1\n",
    "                queue.append(neighbor)\n",
    "                \n",
    "    return distances\n",
    "\n",
    "def generate_distance_matrix():\n",
    "    \"\"\"\n",
    "    Generate a distance matrix for all keys on the keyboard.\n",
    "    \"\"\"\n",
    "    keyboard_graph = build_keyboard_graph()\n",
    "    all_chars = sorted(keyboard_graph.keys())\n",
    "    \n",
    "    distance_matrix = {}\n",
    "    \n",
    "    for char in all_chars:\n",
    "        # Chạy BFS từ mỗi phím để lấy một hàng của ma trận\n",
    "        distance_matrix[char] = bfs_from_start_node(keyboard_graph, char)\n",
    "        \n",
    "    return distance_matrix\n",
    "\n",
    "final_matrix = generate_distance_matrix()\n",
    "\n",
    "# Save the distance matrix to a csv file\n",
    "df = pd.DataFrame(final_matrix)\n",
    "\n",
    "# Sort the DataFrame by index (characters)\n",
    "df = df.reindex(sorted(df.index))\n",
    "\n",
    "df.to_csv(\"keyboard_distances.csv\", index_label='char')\n",
    "print(\"Distance matrix saved to 'keyboard_distances.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a606",
   "metadata": {},
   "source": [
    "# Add keyboard aware suggestion function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f07433a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_distances_from_csv(filename=\"keyboard_distances.csv\"):\n",
    "    \"\"\"\n",
    "    Load the keyboard distance matrix from a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the distances between characters.\n",
    "    \"\"\"\n",
    "    distances = {}\n",
    "\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)[1:] # Skip the first column (character names)\n",
    "        \n",
    "        for row in reader:\n",
    "            char1 = row[0]\n",
    "            char1_distances = {}\n",
    "            for i, dist_str in enumerate(row[1:]):\n",
    "                char2 = header[i]\n",
    "                char1_distances[char2] = int(dist_str)\n",
    "            distances[char1] = char1_distances\n",
    "            \n",
    "    print(f\"Keyboard distances loaded from {filename}.\")\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94a85510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyboard_levenshtein_distance(token1, token2, keyboard_distances):\n",
    "    COST_INSERT = 1.0\n",
    "    COST_DELETE = 1.0\n",
    "    \n",
    "    token1 = token1.lower()\n",
    "    token2 = token2.lower()\n",
    "    \n",
    "    distances = [[0.0] * (len(token2) + 1) for _ in range(len(token1) + 1)]\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = float(t1) * COST_DELETE\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = float(t2) * COST_INSERT\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            char1 = token1[t1 - 1]\n",
    "            char2 = token2[t2 - 1]\n",
    "\n",
    "            if char1 == char2:\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                key_dist = keyboard_distances.get(char1, {}).get(char2, 99)\n",
    "                \n",
    "                if key_dist == 1:\n",
    "                    cost_replace = 0.8\n",
    "                elif key_dist == 2:\n",
    "                    cost_replace = 0.9\n",
    "                else:\n",
    "                    cost_replace = 1.0\n",
    "\n",
    "                cost_del = distances[t1 - 1][t2] + COST_DELETE\n",
    "                cost_ins = distances[t1][t2 - 1] + COST_INSERT\n",
    "                cost_sub = distances[t1 - 1][t2 - 1] + cost_replace\n",
    "                \n",
    "                distances[t1][t2] = min(cost_del, cost_ins, cost_sub)\n",
    "\n",
    "    return distances[len(token1)][len(token2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "678da44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_corrections_improve(token, \n",
    "                        vocabulary,\n",
    "                        keyboard_distances,\n",
    "                        max_suggestions=5,\n",
    "                        max_distance=2): \n",
    "    \"\"\"\n",
    "    Suggest corrections for a given token based on the English vocabulary.\n",
    "    \n",
    "    Return:\n",
    "        List[str]: A list of suggested corrections for the token.\n",
    "    \"\"\"\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in vocabulary:\n",
    "        return [token]\n",
    "\n",
    "    candidates = []\n",
    "    for word in vocabulary:\n",
    "        if abs(len(token_lower) - len(word)) <= max_distance:\n",
    "            dist = keyboard_levenshtein_distance(token_lower, word, keyboard_distances)\n",
    "            if dist <= max_distance:\n",
    "                candidates.append((word, dist))\n",
    "    \n",
    "    candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "    return [word for word, dist in candidates[:max_suggestions]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf34913",
   "metadata": {},
   "source": [
    "# Testing with sample texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard distances loaded from keyboard_distances.csv.\n",
      "Đề xuất cho 'Ameraca': ['america', 'amerce', 'araca', 'maraca', 'amurca']\n",
      "Đề xuất cho 'bsn': ['ban', 'ben', 'bin', 'bosn', 'bon']\n",
      "Đề xuất cho 'hsllo': ['hello', 'hollo', 'sloo', 'callo', 'hall']\n"
     ]
    }
   ],
   "source": [
    "keyboard_distances = load_distances_from_csv(\"keyboard_distances.csv\")\n",
    "\n",
    "for test in test_words:\n",
    "    suggestions = suggest_corrections_improve(test, english_vocabulary, keyboard_distances)\n",
    "    print(f\"Đề xuất cho '{test}': {suggestions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61a612",
   "metadata": {},
   "source": [
    "# Testing with Birkbeck Spelling Error Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cbd5a62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./missp.dat.txt\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "pairs = [] # (misspell, correct)\n",
    "current_correct = None\n",
    "for token in text.split():\n",
    "    if token.startswith('$'):\n",
    "        current_correct = token[1:]\n",
    "    else:\n",
    "        pairs.append((token.lower(), current_correct.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "790fb7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded with 850 words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-1: 100%|██████████| 36133/36133 [02:09<00:00, 278.31it/s] \n",
      "Evaluating Top-5: 100%|██████████| 36133/36133 [02:10<00:00, 277.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 4.104%\n",
      "Top-5 accuracy: 5.975%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocabulary = load_english_vocabulary('en-basic')\n",
    "\n",
    "def evaluate_improve(pairs, suggester, topk):\n",
    "    hits = 0\n",
    "    for mis, cor in tqdm(pairs, desc=f\"Evaluating Top-{topk}\"):\n",
    "        suggestions = suggester(mis, vocabulary, keyboard_distances,\n",
    "                                max_suggestions=topk, max_distance=2)\n",
    "        top_suggestions = [s.lower() for s in suggestions]\n",
    "        if cor in top_suggestions[:topk]:\n",
    "            hits += 1\n",
    "    return hits / len(pairs)\n",
    "\n",
    "top1 = evaluate_improve(pairs, suggest_corrections_improve, 1)\n",
    "top5 = evaluate_improve(pairs, suggest_corrections_improve, 5)\n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.3%}\")\n",
    "print(f\"Top-5 accuracy: {top5:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f361e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Top-1: 100%|██████████| 36133/36133 [01:22<00:00, 438.36it/s] \n",
      "Evaluating Top-5: 100%|██████████| 36133/36133 [01:23<00:00, 434.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy: 4.173%\n",
      "Top-5 accuracy: 5.978%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(pairs, suggester, topk):\n",
    "    hits = 0\n",
    "    for mis, cor in tqdm(pairs, desc=f\"Evaluating Top-{topk}\"):\n",
    "        suggestions = suggester(mis, vocabulary,\n",
    "                                max_suggestions=topk, max_distance=2)\n",
    "        top_suggestions = [s.lower() for s in suggestions]\n",
    "        if cor in top_suggestions[:topk]:\n",
    "            hits += 1\n",
    "    return hits / len(pairs)\n",
    "\n",
    "top1 = evaluate(pairs, suggest_corrections, 1)\n",
    "top5 = evaluate(pairs, suggest_corrections, 5)\n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.3%}\")\n",
    "print(f\"Top-5 accuracy: {top5:.3%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
